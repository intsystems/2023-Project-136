\documentclass{article}
\usepackage{customized_arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath, amsfonts, amssymb, amsthm, mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{assumption}
\newtheorem{assumption}{Assumption}[section]



\title{Stochastic Newton with Arbitrary Sampling}

\author{Denis Shveykin
	%% examples of more authors
	\And
	Rustem Islamov
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{Stochastic Newton with Arbitrary Sampling}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Stochastic Newton with Arbitrary Sampling},
pdfsubject={},
pdfauthor={Denis Shveykin, Rustem Islamov},
pdfkeywords={},
}

\begin{document}
	
\maketitle

\begin{abstract}
	
	The problem of minimizing the average of a large number of sufficiently smooth and strongly convex functions is ubiquitous in machine learning. Stochastic first-order methods for this problem of Stochastic Gradient Descent type are well studied. In turn, second-order methods, such as Newton, have certain advances since they can adapt to the curvature of the problem. They are also known for their fast convergence rates. But stochastic variants of Newton-type methods are not studied as good as SGD-type ones and have limitations on the batch size. \cite{kovalev2019stochastic} proposed a method which requires no limitations on batch sizes. Our goal is to explore this method with different sampling strategies that lead to practical improvements.
	
\end{abstract}


\keywords{Stochastic Newton, sampling strategy}

\section{Introduction}

	The problem is to minimize the empirical risk which has finite-sum structure \cite{kovalev2019stochastic}:
	\begin{equation}\label{ERM}
		\underset{x \in \mathbb R^d}{\min} \left[ f(x) \overset{def}{=} \frac{1}{n} \sum \limits_{i=1}^n f_i(x) \right]
	\end{equation}
	where each $f_i$ is assumed to have Lipschitz Hessian. \\
	
	Typically, $n$ is very large for modern real-world problems. Thus, the stochastic approach is used because it is computationally difficult to evaluate the gradient of all $f_i$ at each step. The Stochastic Gradient Descent (SGD) method \cite{SGD-1} calculates the gradients of some randomly chosen $f_i$ which leads to cheaper computation per-iteration cost compared to the vanilla Gradient Descent (GD). The analysis of SGD and its modifications is rich and well explored. Firstly, the theory of SGD-type methods does not restrict the batch size. Therefore, these algorithms can be applied even with small batches. It is known that simple SGD converges only to a neighbourhood of the solution only \cite{sgd-hogwild, sgd-general-analysis} whose size is proportional to the variance of the stochastic gradient. However, there are techniques (aka variance reduction) to solve this issue. These techniques \cite{exp-convergence, advances-NIPS, unified-sgd, one-method} modify the update rule of vanilla SGD which allows to mitigate the aforementioned effect without changing per-iteration cost. However, the main disadvantage of all gradient-type methods is that a computation complexity depends on the curvature of the problem, which is called the condition number and is defined as the ratio of Lipschitzness and strong convexity parameters. \\
	
	This is a place where second-order methods such as the Newton method \cite{Nesterov-introductory, Newton-convergence, RSN} come to play. Taking into account second-order derivatives it possible to adjust the algorithm's step sizes to the curvature of the problem \cite{Nesterov-introductory}. Unfortunately, much less work has been done in the direction of stochastic Newton-type methods. Many algorithms \cite{sub-sampled, exact-inexact, variance-reduced-Newton, zhang2022adaptive, tripuraneni2018stochastic, zhou2020stochastic} require large batch sizes. In particular, the required batch size is commonly quadratically proportional to the inverse of the desired accuracy. That means that one need to evaluate a large number of $f_i$ Hessians which sometimes can be much larger than $n$. \cite{kovalev2019stochastic} proposes a simple Stochastic Newton algorithm, which can work with batches of any size. Their algorithms achieve local linear and super-linear convergence. \\
	
	In practice, various sampling strategies are used for SGD-type algorithms to improve further the performance. One of the most famous sampling mechanisms is so-called Importance Sampling \cite{https://doi.org/10.48550/arxiv.1401.2753, 9413313}. The idea is to compute gradients of the functions that have more impact on the problem. \cite{richtarik2016parallel} studies many other sampling mechanisms. We aim to analyse such strategies, but for Algorithms $1$ and $2$ of \cite{kovalev2019stochastic} to improve the theoretical and practical applications of algorithms. We investigate various sampling strategies supporting them with  rigorously constructed experiments. 
	
\section{Problem statement}

	We discuss the problem of empirical risk minimization (ERM). The objective function $f$ (\ref{ERM}) is the average of a big number of functions $f_i$. In fact each $f_i$ is the loss on the $i$-th train data point. For example, this notation can be attributed to linear regression. In this case we try to fit the model's parameter vector $x$ to minimize the mean squared error (MSE) on the train data. \\
	
	\subsection{Assumptions}
	
	In our work we will assume the functions $f_i$ to satisfy the same regularity conditions that were originally introduced in \cite{kovalev2019stochastic}.
	
	
	\begin{assumption}[Strong convexity] A differentiable function $\phi:\ \mathbb R^d \rightarrow \mathbb R$ is $\mu$-strongly convex, where $\mu > 0$, if $\forall\ x, y \in \mathbb R^d$
	
	\begin{equation}\label{strong-conv}
		\phi(x) \geqslant \phi(y) + \langle \nabla \phi, x - y \rangle + \frac{\mu}{2} \| x - y \|^2,
	\end{equation}
	\end{assumption}
	where the norm $\| \cdot \|$ is Euclidean. For twice differentiable functions this assumption is equivalent to the Hessian having each eigenvalue $\geqslant \mu$.
	
	\begin{assumption}[Lipschitz Hessian] A function $\phi: \mathbb R^d \rightarrow R$ has H-Lipschitz Hessian if $\forall\ x, y \in \mathbb R^d$
	
	\begin{equation}\label{lip-hess}
		\| \nabla^2 \phi(x) - \nabla^2 \phi(y) \| \leqslant \| x - y \|
	\end{equation}
	\end{assumption}

	\subsection{Sampling}
	
	\begin{definition}[Sampling]
		A random set-valued mapping $\hat S:\ [n] \rightarrow 2^{[n]}$ is called sampling.
	\end{definition}

	That means that each $S_k \subseteq [n]$ is a realization of $\hat S$. In this case we can call any particular probability distribution on $2^{[n]}$ a sampling strategy.
	\subsection{Algorithm}
	
	We will be applying different sampling strategies for the Algorithm 1 from \cite{kovalev2019stochastic}:
	
	\begin{algorithm}
		\caption{Stochastic Newton (SN)}\label{SN}
		\begin{algorithmic}
			\item \textbf{Initialize:} Choose starting iterates $w_1^0, w_2^0, ... w_n^0 \in \mathbb R^d$
			
			\item \For {$k = 0, 1, 2, ...$}	
			
				$ x^{k+1} = \left[ \frac{1}{n} \sum \limits_{i=1}^n \nabla^2 f_i(w_i^k) \right]^{-1} \left[ \frac{1}{n} \sum \limits_{i=1}^n \nabla^2 f_i(w_i^k) w_i^k - \nabla f_i(w_i^k) \right] $
				
				Choose a subset $S^k \subseteq \{ 1, 2, ..., n \}$ with one of the sampling strategies
				
				$w_i^{k+1} = 
				\begin{cases}
					x^{k+1} & i \in S^k \\
					w_i^k & i \notin S^k
				\end{cases}$
				
			\item \EndFor
		\end{algorithmic}
	\end{algorithm}

	\subsection{Goal of the project}
	
	We aim to explore different sampling strategies for the Algorithm \ref{SN} in order to obtain practical improvements and their theoretical explainations.
	
% references title
\renewcommand\refname{References}	
\bibliographystyle{plain}
\bibliography{references}

\end{document}
