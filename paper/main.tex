\documentclass{article}
\usepackage{customized_arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath, amsfonts, amssymb, amsthm, mathtools}



\title{Stochastic Newton with Arbitrary Sampling}

\author{Denis Shveykin
	%% examples of more authors
	\And
	Rustem Islamov
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{Stochastic Newton with Arbitrary Sampling}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Stochastic Newton with Arbitrary Sampling},
pdfsubject={},
pdfauthor={Denis Shveykin, Rustem Islamov},
pdfkeywords={},
}

\begin{document}
	
\maketitle

\begin{abstract}
	
	The problem of minimizing the average of a large number sufficiently smooth and strongly convex functions is ubiquitous in machine learning. Stochastic first-order methods for this problem of Stochastic Gradient Descent type are well studied. In turn, second-order methods, such as Newton, have certain advances since they can adapt to the curvature of the problem. They are also known for their fast convergence rates. But stochastic variants Newton-type methods are not well-studied and have limitations on the batch size. Dmitry Kovalev et al proposed a method which requires no limitations on batch sizes. Our goal is to explore this method with different sampling strategies that lead to practical improvements.
	
\end{abstract}


\keywords{}

\section{Introduction}

	The problem is to minimize the empirical risk which has finite-sum structure \cite{kovalev}:
	\begin{equation}
		\underset{x \in \mathbb R^d}{\min} \left[ f(x) \overset{def}{=} \frac{1}{n} \sum \limits_{i=1}^n f_i(x) \right]
	\end{equation}
	where each $f_i$ is assumed to have Lipschitz Hessian. \\
	
	The stochastic approach is used, because $n$ is very large and it is computationally difficult to evaluate the gradient of each $f_i$ on each step. The Stochastic Gradient Descent (SGD) method \cite{SGD-1} calculates the gradients of some randomly chosen $f_i$, which has cheap iterations compared to the deterministic algorithm. Such methods have two certain advantages. Firstly, they do not rely on the number of used $f_i$ on a single step, which is called the batch size. Therefore, these methods can be applied even with small batches. \\
	
	Secondly, there are ways to solve some common problem that arises due to the stochastic nature of the methods. The fact is that stochastic methods provide convergence to a neighbourhood of the solution only \cite{sgd-hogwild, sgd-general-analysis}. Its size is proportional to the variance of the stochastic gradient. So the second advantage of SGD is presence of so-called variance-reduced methods \cite{exp-convergence, advances-NIPS, unified-sgd, one-method}, which mitigate the mentioned effect. However, these methods' computational complexity depends on the curvature of the problem, which is called the condition number and is defined as the ratio of Lipschitzness and strong convexity parameters. \\
	
	This leads to usage of second-order methods, such as the Newton method. Taking into account the derivatives of the second order makes it possible to adjust the algorithm's step sizes to the curvature of the problem. Unfortunately, much less literature has been written on this topic than about first-order methods. Some proposed algorithms need extra assumptions or regularization to converge. Some works provide stochastic Newton-type methods, whose computational complexity exceeds such in the variance-reduced SGD variants. \\
	
	In addition, there are presented many algorithms that need large batch sizes. Particularly, the required batch size is commonly quadratically proportional to the inverse of the desired accuracy. That means that one need to evaluate a large number of $f_i$ Hessians. And this decreases the profit gained by adding randomness into the algorithm, because these batch sizes can become as big as $n$. \\
	
	Dmitry Kovalev, Konstantin Mishchenko and Peter Richtarik proposed a simple Stochastic Newton algorithm, which can work with small batches, even with batches of size one. Their algorithm does not provide unbiased estimates but nevertheless shows good convergence. This is achieved by developing new Lyapunov functions that are specific to second-order methods. Our goal is to apply different sampling strategies to this method and explore their performance. \\
	
	The basic set of sampling strategies can be taken from [parallel CDM], where these strategies are applied to the Parallel Coordinate Descent methods. It is also known that so-called Importance Sampling can improve the SGD performance, since it can reduce the stochastic gradient variance. \\
	
	
	
% references title
\renewcommand\refname{References}	
\bibliographystyle{plain}
\bibliography{references}

\end{document}