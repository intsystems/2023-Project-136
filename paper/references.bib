@article{kovalev2019stochastic,
  title={Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates},
  author={Kovalev, Dmitry and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1912.01597},
  year={2019}
}


@inproceedings{gower2019sgd,
	title={SGD: General analysis and improved rates},
	author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
	booktitle={International conference on machine learning},
	pages={5200--5209},
	year={2019},
	organization={PMLR}
}


@article{mishchenko2020random,
	title={Random reshuffling: Simple analysis with vast improvements},
	author={Mishchenko, Konstantin and Khaled, Ahmed and Richt{\'a}rik, Peter},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={17309--17320},
	year={2020}
}


@article{SGD-1,
author = {Herbert Robbins and Sutton Monro},
title = {{A Stochastic Approximation Method}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {400 -- 407},
year = {1951},
doi = {10.1214/aoms/1177729586},
URL = {https://doi.org/10.1214/aoms/1177729586}
}



@InProceedings{sgd-hogwild,
  title = 	 {{SGD} and Hogwild! {C}onvergence Without the Bounded Gradients Assumption},
  author =       {Nguyen, Lam and NGUYEN, PHUONG HA and van Dijk, Marten and Richtarik, Peter and Scheinberg, Katya and Takac, Martin},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3750--3758},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/nguyen18c/nguyen18c.pdf},
  url = 	 {https://proceedings.mlr.press/v80/nguyen18c.html},
  abstract = 	 {Stochastic gradient descent (SGD) is the optimization algorithm of choice in many machine learning applications such as regularized empirical risk minimization and training deep neural networks. The classical convergence analysis of SGD is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is always violated for cases where the objective function is strongly convex. In (Bottou et al.,2016), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. Here we show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime, which results in more relaxed conditions than those in (Bottou et al.,2016). We then move on the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime, obtaining the first convergence results for this method in the case of diminished learning rate.}
}


@article{sgd-general-analysis,
  doi = {10.48550/ARXIV.1901.09401},
  
  url = {https://arxiv.org/abs/1901.09401},
  
  author = {Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richtarik, Peter},
  
  keywords = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {SGD: General Analysis and Improved Rates},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{exp-convergence,
 author = {Roux, Nicolas and Schmidt, Mark and Bach, Francis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Stochastic Gradient Method with an Exponential Convergence \_Rate for Finite Training Sets},
 url = {https://proceedings.neurips.cc/paper/2012/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf},
 volume = {25},
 year = {2012}
}


@inproceedings{advances-NIPS,
 author = {Johnson, Rie and Zhang, Tong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Accelerating Stochastic Gradient Descent using Predictive Variance Reduction},
 url = {https://proceedings.neurips.cc/paper/2013/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf},
 volume = {26},
 year = {2013}
}


@misc{unified-sgd,
  doi = {10.48550/ARXIV.1905.11261},
  
  url = {https://arxiv.org/abs/1905.11261},
  
  author = {Gorbunov, Eduard and Hanzely, Filip and Richtárik, Peter},
  
  keywords = {Optimization and Control (math.OC), Machine Learning (cs.LG), Numerical Analysis (math.NA), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{one-method,
  doi = {10.48550/ARXIV.1905.11266},
  
  url = {https://arxiv.org/abs/1905.11266},
  
  author = {Hanzely, Filip and Richtárik, Peter},
  
  keywords = {Optimization and Control (math.OC), Machine Learning (cs.LG), Numerical Analysis (math.NA), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {One Method to Rule Them All: Variance Reduction for Data, Parameters and Many New Methods},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{Nesterov-introductory,
	author = {Nesterov, Yurii},
	title = {Introductory lectures on convex optimization: a basic course/ by Yurii Nesterov},
	publisher = {Kluwer Academic Publishers},
	year = {2004},
	series = {Applied optimization, 87},
	address = {Boston}
}


@misc{Newton-convergence,
  doi = {10.48550/ARXIV.1806.00413},
  
  url = {https://arxiv.org/abs/1806.00413},
  
  author = {Karimireddy, Sai Praneeth and Stich, Sebastian U. and Jaggi, Martin},
  
  keywords = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics, G.1.6, 90C25, 68Q25},
  
  title = {Global linear convergence of Newton's method without strong-convexity or Lipschitz gradients},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{RSN,
 author = {Gower, Robert and Kovalev, Dmitry and Lieder, Felix and Richtarik, Peter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {RSN: Randomized Subspace Newton},
 url = {https://proceedings.neurips.cc/paper/2019/file/bc6dc48b743dc5d013b1abaebd2faed2-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{cubic-regularization,
  author    = {Yurii E. Nesterov and
               Boris T. Polyak},
  title     = {Cubic regularization of Newton method and its global performance},
  journal   = {Math. Program.},
  volume    = {108},
  number    = {1},
  pages     = {177--205},
  year      = {2006},
  url       = {https://doi.org/10.1007/s10107-006-0706-8},
  doi       = {10.1007/s10107-006-0706-8},
  timestamp = {Thu, 24 Oct 2019 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/journals/mp/NesterovP06.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@InProceedings{randomized-cubic,
  title = 	 {Randomized Block Cubic {N}ewton Method},
  author =       {Doikov, Nikita and Richtarik, Peter and of Edinburgh, University},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1290--1298},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/doikov18a/doikov18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/doikov18a.html},
  abstract = 	 {We study the problem of minimizing the sum of three convex functions: a differentiable, twice-differentiable and a non-smooth term in a high dimensional setting. To this effect we propose and analyze a randomized block cubic Newton (RBCN) method, which in each iteration builds a model of the objective function formed as the sum of the natural models of its three components: a linear model with a quadratic regularizer for the differentiable term, a quadratic model with a cubic regularizer for the twice differentiable term, and perfect (proximal) model for the nonsmooth term. Our method in each iteration minimizes the model over a random subset of blocks of the search variable. RBCN is the first algorithm with these properties, generalizing several existing methods, matching the best known bounds in all special cases. We establish ${\cal O}(1/\epsilon)$, ${\cal O}(1/\sqrt{\epsilon})$ and ${\cal O}(\log (1/\epsilon))$ rates under different assumptions on the component functions. Lastly, we show numerically that our method outperforms the state-of-the-art on a variety of machine learning problems, including cubically regularized least-squares, logistic regression with constraints, and Poisson regression.}
}


@article{uniform-convex-cubic,
	doi = {10.1007/s10957-021-01838-7},
  
	url = {https://doi.org/10.1007%2Fs10957-021-01838-7},
  
	year = 2021,
	month = {mar},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {189},
  
	number = {1},
  
	pages = {317--339},
  
	author = {Nikita Doikov and Yurii Nesterov},
  
	title = {Minimizing Uniformly Convex Functions by Cubic Regularization of Newton Method},
  
	journal = {Journal of Optimization Theory and Applications}
}


@misc{proximal-quasi,
  doi = {10.48550/ARXIV.1602.00223},
  
  url = {https://arxiv.org/abs/1602.00223},
  
  author = {Luo, Luo and Chen, Zihao and Zhang, Zhihua and Li, Wu-Jun},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Proximal Stochastic Quasi-Newton Algorithm},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@InProceedings{lbfgs,
  title = 	 {A Linearly-Convergent Stochastic L-BFGS Algorithm},
  author = 	 {Moritz, Philipp and Nishihara, Robert and Jordan, Michael},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--258},
  year = 	 {2016},
  editor = 	 {Gretton, Arthur and Robert, Christian C.},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/moritz16.pdf},
  url = 	 {https://proceedings.mlr.press/v51/moritz16.html},
  abstract = 	 {We propose a new stochastic L-BFGS algorithm and prove a linear convergence rate for strongly convex and smooth functions. Our algorithm draws heavily from a recent stochastic variant of L-BFGS proposed in Byrd et al. (2014) as well as a recent approach to variance reduction for stochastic gradient descent from Johnson and Zhang (2013). We demonstrate experimentally that our algorithm performs well on large-scale convex and non-convex optimization problems, exhibiting linear convergence and rapidly solving the optimization problems to high levels of precision. Furthermore, we show that our algorithm performs well for a wide-range of step sizes, often differing by several orders of magnitude.}
}



@InProceedings{stoch-bfgs,
  title = 	 {Stochastic Block BFGS: Squeezing More Curvature out of Data},
  author = 	 {Gower, Robert and Goldfarb, Donald and Richtarik, Peter},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1869--1878},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gower16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/gower16.html},
  abstract = 	 {We propose a novel limited-memory stochastic block BFGS update for incorporating enriched curvature information in stochastic approximation methods. In our method, the estimate of the inverse Hessian matrix that is maintained by it, is updated at each iteration using a sketch of the Hessian, i.e., a randomly generated compressed form of the Hessian. We propose several sketching strategies, present a new quasi-Newton method that uses stochastic block BFGS updates combined with the variance reduction approach SVRG to compute batch stochastic gradients, and prove linear convergence of the resulting method. Numerical tests on large-scale logistic regression problems reveal that our method is more robust and substantially outperforms current state-of-the-art methods.}
}



@InProceedings{sub-sampled,
  title = 	 {Sub-sampled Cubic Regularization for Non-convex Optimization},
  author =       {Jonas Moritz Kohler and Aurelien Lucchi},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1895--1904},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/kohler17a/kohler17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/kohler17a.html},
  abstract = 	 {We consider the minimization of non-convex functions that typically arise in machine learning. Specifically, we focus our attention on a variant of trust region methods known as cubic regularization. This approach is particularly attractive because it escapes strict saddle points and it provides stronger convergence guarantees than first- and second-order as well as classical trust region methods. However, it suffers from a high computational complexity that makes it impractical for large-scale learning. Here, we propose a novel method that uses sub-sampling to lower this computational cost. By the use of concentration inequalities we provide a sampling scheme that gives sufficiently accurate gradient and Hessian approximations to retain the strong global and local convergence guarantees of cubically regularized methods. To the best of our knowledge this is the first work that gives global convergence guarantees for a sub-sampled variant of cubic regularization on non-convex functions. Furthermore, we provide experimental results supporting our theory.}
}


@article{exact-inexact,
    author = {Bollapragada, Raghu and Byrd, Richard H and Nocedal, Jorge},
    title = "{Exact and inexact subsampled Newton methods for optimization}",
    journal = {IMA Journal of Numerical Analysis},
    volume = {39},
    number = {2},
    pages = {545-578},
    year = {2018},
    month = {04},
    abstract = "{The paper studies the solution of stochastic optimization problems in which approximations to the gradient and Hessian are obtained through subsampling. We first consider Newton-like methods that employ these approximations and discuss how to coordinate the accuracy in the gradient and Hessian to yield a superlinear rate of convergence in expectation. The second part of the paper analyzes an inexact Newton method that solves linear systems approximately using the conjugate gradient (CG) method, and that samples the Hessian and not the gradient (the gradient is assumed to be exact). We provide a complexity analysis for this method based on the properties of the CG iteration and the quality of the Hessian approximation, and compare it with a method that employs a stochastic gradient iteration instead of the CG method. We report preliminary numerical results that illustrate the performance of inexact subsampled Newton methods on machine learning applications based on logistic regression.}",
    issn = {0272-4979},
    doi = {10.1093/imanum/dry009},
    url = {https://doi.org/10.1093/imanum/dry009},
    eprint = {https://academic.oup.com/imajna/article-pdf/39/2/545/28378184/dry009.pdf},
}


@inproceedings{variance-reduced-Newton,
  title={Stochastic variance-reduced cubic regularized Newton methods},
  author={Zhou, Dongruo and Xu, Pan and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={5990--5999},
  year={2018},
  organization={PMLR}
}


@article{zhang2022adaptive,
  title={Adaptive stochastic variance reduction for subsampled Newton method with cubic regularization},
  author={Zhang, Junyu and Xiao, Lin and Zhang, Shuzhong},
  journal={INFORMS Journal on Optimization},
  volume={4},
  number={1},
  pages={45--64},
  year={2022},
  publisher={INFORMS}
}


@article{tripuraneni2018stochastic,
  title={Stochastic cubic regularization for fast nonconvex optimization},
  author={Tripuraneni, Nilesh and Stern, Mitchell and Jin, Chi and Regier, Jeffrey and Jordan, Michael I},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}


@inproceedings{zhou2020stochastic,
  title={Stochastic recursive variance-reduced cubic regularization methods},
  author={Zhou, Dongruo and Gu, Quanquan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3980--3990},
  year={2020},
  organization={PMLR}
}


@article{richtarik2016parallel,
  title={Parallel coordinate descent methods for big data optimization},
  author={Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={Mathematical Programming},
  volume={156},
  pages={433--484},
  year={2016},
  publisher={Springer}
}


@misc{https://doi.org/10.48550/arxiv.1401.2753,
  doi = {10.48550/ARXIV.1401.2753},
  
  url = {https://arxiv.org/abs/1401.2753},
  
  author = {Zhao, Peilin and Zhang, Tong},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Stochastic Optimization with Importance Sampling},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@INPROCEEDINGS{9413313,
  author={Liu, Huikang and Wang, Xiaolu and Li, Jiajin and So, Anthony Man-Cho},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Low-Cost Lipschitz-Independent Adaptive Importance Sampling of Stochastic Gradients}, 
  year={2021},
  volume={},
  number={},
  pages={2150-2157},
  doi={10.1109/ICPR48806.2021.9413313}}
